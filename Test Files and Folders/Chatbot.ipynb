{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/Desktop/My Projects/Chatbot-Development-with-Python-for-AI-ML/ApplabQatar/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to /home/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_2Wf51V_3db8TfcJj91FkyhKNPhDwainW3yWC2ErC3z4T8hoLx8fowaFYfhAbLS5TK22n1q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229112/3548520120.py:31: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "import fitz  # PyMuPDF\n",
    "from pinecone import Pinecone\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "app = FastAPI()\n",
    "YOUR_PINECONE_API_KEY = os.getenv(\"PINCECONE_API_KEYS\")\n",
    "print(YOUR_PINECONE_API_KEY)\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_2Wf51V_3db8TfcJj91FkyhKNPhDwainW3yWC2ErC3z4T8hoLx8fowaFYfhAbLS5TK22n1q\")\n",
    "index = pc.Index(\"applabqatar\")\n",
    "# index = pinecone.Index(\"applabqatar\")\n",
    "\n",
    "# Initialize OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739486621&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTQ4NjYyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=XotXGoqZLcIOWqP6pN8W0EbVETdlaLXmn9cNeqBi06VbSuYFnhDcS5DT88enK%7ETymKjCJ9XHAO8JaUmKOw%7E%7EeCopMS7GX3P%7EV7lW49cY6W44f4BT9VTe3d-M68-bmaDon3i3RvMi4PLTNOcuKLLiFOE-vDKGZTyOaoGBjvYzI1osMl8N6J4J%7EtHtRXxLH17a23dLlYv6p6nlBhcUgAJTPXXswvCijrkDPapnIitcBMCvxLqen6jfRQwZSaolo2Qo6aXaLBPVW3XgOlMat5TR7ZFQtsbRi2mMtKc1IWXTtJkFu-MBKrhQNJORvOrtIgtCPvdFixCTgHjxhAl%7E1djvlg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Apply NLP preprocessing (Tokenization, Stopword Removal, Lemmatization)\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum()]\n",
    "    tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    \"\"\"Extract key Named Entities from text using Spacy\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from the PDF and apply summarization\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "    # Summarize long text\n",
    "    if len(text) > 1000:\n",
    "        text = summarizer(text[:1024], max_length=300, min_length=100, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "    return preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/upload/\")\n",
    "async def upload_pdf(file: UploadFile = File(...)):\n",
    "    pdf_path = f\"temp_{file.filename}\"\n",
    "    with open(pdf_path, \"wb\") as buffer:\n",
    "        buffer.write(await file.read())\n",
    "\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Chunk and embed text\n",
    "    sentences = sent_tokenize(text)\n",
    "    doc_embeddings = [embeddings.embed_query(sent) for sent in sentences]\n",
    "\n",
    "    # Store in Pinecone\n",
    "    for i, vector in enumerate(doc_embeddings):\n",
    "        index.upsert([(f\"{file.filename}_{i}\", vector, {\"text\": sentences[i]})])\n",
    "\n",
    "    return {\"message\": \"PDF uploaded and processed successfully!\", \"entities\": extract_named_entities(text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_text_with_tfidf(query, texts):\n",
    "    \"\"\"Rank retrieved text chunks based on TF-IDF relevance\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([query] + texts)\n",
    "    scores = tfidf_matrix[0].dot(tfidf_matrix.T).toarray()[0][1:]\n",
    "    ranked_texts = [texts[i] for i in sorted(range(len(scores)), key=lambda x: scores[x], reverse=True)]\n",
    "    return \" \".join(ranked_texts[:3])  # Top 3 ranked chunks\n",
    "\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat(query: str):\n",
    "    results = index.query(query, top_k=5, include_metadata=True)\n",
    "    retrieved_texts = [r[\"metadata\"][\"text\"] for r in results[\"matches\"]]\n",
    "\n",
    "    # Rank retrieved texts using TF-IDF\n",
    "    context = rank_text_with_tfidf(query, retrieved_texts)\n",
    "\n",
    "    chat_model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    response = chat_model.predict(f\"Context: {context} \\n\\nAnswer the question: {query}\")\n",
    "\n",
    "    return {\"response\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 03:20:11.550 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.592 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/ali/Desktop/My Projects/Chatbot-Development-with-Python-for-AI-ML/ApplabQatar/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-14 03:20:11.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.596 Session state does not function when running a script without `streamlit run`\n",
      "2025-02-14 03:20:11.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-14 03:20:11.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "st.title(\"PDF Chatbot with NLP\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload a PDF\", type=[\"pdf\"])\n",
    "if uploaded_file:\n",
    "    files = {\"file\": uploaded_file.getvalue()}\n",
    "    response = requests.post(\"http://127.0.0.1:8000/upload/\", files=files)\n",
    "    data = response.json()\n",
    "    st.success(data[\"message\"])\n",
    "    st.write(\"Extracted Named Entities:\", data[\"entities\"])\n",
    "\n",
    "query = st.text_input(\"Ask a question:\")\n",
    "if query:\n",
    "    response = requests.post(\"http://127.0.0.1:8000/chat/\", json={\"query\": query})\n",
    "    st.write(\"Chatbot:\", response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ApplabQatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
